{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from FibCoder import FibCoder\n",
    "import os\n",
    "import codecs\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from multiprocessing.dummy import Lock as ThreadLock \n",
    "from collections import defaultdict\n",
    "from multiprocessing.dummy import Value as ThreadValue\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustration(func):\n",
    "    \"\"\"\n",
    "    Распаралеливание выкачки страниц.\n",
    "    \"\"\"\n",
    "    mutex = ThreadLock()\n",
    "    n_thread = ThreadValue('i',0)\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **argv):\n",
    "        result = func(*args, **argv)\n",
    "        with mutex:\n",
    "            nonlocal n_thread\n",
    "            n_thread.value +=1\n",
    "            print(f\"\\r{n_thread.value} objects are processed...\",end ='',flush = True)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_queries.pkl','rb') as f:\n",
    "    id_queries = pickle.load(f)\n",
    "with open('final_text.pkl','rb') as f:\n",
    "    final_text = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_urls = {}\n",
    "with codecs.open('urls.numerate.txt') as f:\n",
    "    for line in f:\n",
    "        new_line = line[:-1].split('\\t')\n",
    "        id_urls[new_line[0]] = new_line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url_docname.pkl','rb') as f:\n",
    "    url_docname = pickle.load(f)\n",
    "with open('docid_urls.pkl','rb') as f:\n",
    "    id_urls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_docname['kak-gta-san-andreas-sest-na-perednee-sidene.blogbuzz.pp.ru'] = 'content/content/20170702/doc.0087.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id_docs.pkl','rb') as f:\n",
    "    id_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@illustration\n",
    "def w_1(ad):\n",
    "        try:\n",
    "            score_docs = defaultdict(lambda:defaultdict(dict))\n",
    "            docs = id_docs[ad]\n",
    "            query = id_queries[ad]\n",
    "            words = query.split()\n",
    "            l_r = len(words)\n",
    "            dop_words = [' '.join([words[i],words[i+1]]) for i in range(len(words)-1)]\n",
    "            words += dop_words\n",
    "            phrase = {}\n",
    "            for doc in docs:\n",
    "                for word in query.split():\n",
    "                    text = final_text[url_docname[id_urls[doc]][16:]]\n",
    "                    n = len(re.findall(word,text))\n",
    "                    d_l = len(text)\n",
    "                    if ' ' in word:\n",
    "                        w_s = word.split()\n",
    "                        n += len(re.findall(' '.join(w_s[::-1]),text))*0.5\n",
    "                        n += len(re.findall(w_s[0] + '\\w+' +w_s[1],text))*0.5\n",
    "                    score_docs[doc][word] = {'n':n,'dl':d_l}\n",
    "                n = len(re.findall(query,text))\n",
    "                phrase[doc] = {'n':n,'dl':d_l}\n",
    "            cf = defaultdict(int)\n",
    "            doc_s = defaultdict(float)\n",
    "            for word in query.split():\n",
    "                for doc in docs:\n",
    "                    cf[word] += score_docs[doc][word]['n']\n",
    "            for doc in docs:\n",
    "                N = 0\n",
    "                words_ps = 0\n",
    "                for word in query.split():\n",
    "                    if ' ' not in word:\n",
    "                        if score_docs[doc][word]['n'] == 0:\n",
    "                            N += 1\n",
    "                        c = (-1.5)*cf[word]/score_docs[doc][word]['dl']\n",
    "                        p = math.log(1 - math.exp(c) + 0.0001)\n",
    "                        tf = score_docs[doc][word]['n']/score_docs[doc][word]['dl']\n",
    "                        doc_s[doc] += p * tf/(tf + 1 + 1/350*score_docs[doc][word]['dl'])\n",
    "                        words_ps += p\n",
    "                    else:\n",
    "                        w_s = word.split()\n",
    "                        c_1 = (-1.5)*cf[w_s[0]]/score_docs[doc][w_s[0]]['dl']\n",
    "                        p_1 = math.log(1 - math.exp(c_1))\n",
    "                        c_2 = (-1.5)*cf[w_s[1]]/score_docs[doc][w_s[1]]['dl']\n",
    "                        p_2 = math.log(1 - math.exp(c_2))\n",
    "                        tf = score_docs[doc][word]['n']/score_docs[doc][word]['dl']\n",
    "                        doc_s[doc] += 0.3*(p1 + p2) * tf/(tf + 1)\n",
    "                        words_ps += p\n",
    "                doc_s[doc] += 0.2*words_ps * 0.03**(l_r - N)\n",
    "                tf = phrase[doc]['n']/phrase[doc]['dl']\n",
    "                doc_s[doc] += 0.1*words_ps * tf/(tf + 1)\n",
    "            res[ad] = doc_s\n",
    "        except:\n",
    "            mistakes.append(ad)\n",
    "            print('!ERROR!',ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def their_bm(ad):\n",
    "    docs = id_docs[ad]\n",
    "    corpus = []\n",
    "    for doc in tqdm(docs):\n",
    "        corpus.append(final_text[url_docname[id_urls[doc]][16:]].split())\n",
    "    \n",
    "    bm25 = BM25Okapi(corpus)   \n",
    "    scoring = bm25.get_scores(id_queries[ad].split())\n",
    "    return scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "mistakes = []\n",
    "with ThreadPool(10) as pool: \n",
    "    pool.map(w_1, range(1,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_text_yandex.pkl','wb') as f:\n",
    "    pickle.dump(res,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,400):\n",
    "    if i not in res.keys():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('sub.txt','w') as f:\n",
    "#     f.write('QueryId,DocumentId\\n')\n",
    "#     for i in range(1,400):\n",
    "#         for item in res[i]:\n",
    "#             f.write('{},{}\\n'.format(i,item[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
