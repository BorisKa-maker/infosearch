{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from pyaspeller import YandexSpeller\n",
    "import codecs\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from multiprocessing.dummy import Lock as ThreadLock \n",
    "from multiprocessing.dummy import Value as ThreadValue\n",
    "import functools\n",
    "from string import punctuation\n",
    "from re import escape\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "stemmerR = SnowballStemmer(\"russian\")\n",
    "stemmerE = SnowballStemmer(\"english\")\n",
    "stop_words = set(nltk.corpus.stopwords.words([\"russian\", \"english\"]))\n",
    "punctuation = punctuation + '«»©—.'\n",
    "def illustration(func):\n",
    "    \"\"\"\n",
    "    Распаралеливание выкачки страниц.\n",
    "    \"\"\"\n",
    "    mutex = ThreadLock()\n",
    "    n_thread = ThreadValue('i',0)\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **argv):\n",
    "        result = func(*args, **argv)\n",
    "        with mutex:\n",
    "            nonlocal n_thread\n",
    "            n_thread.value +=1\n",
    "            print(f\"\\r{n_thread.value} objects are processed...\",end ='',flush = True)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def dopparse(ad1):\n",
    "    file = 'content/content/{}'.format(ad1)\n",
    "    request = doc_to_text[file]\n",
    "    result = [morph.parse(word)[0].normal_form for word in request]\n",
    "    final_text[file] = ' '.join(result)\n",
    "@illustration\n",
    "def final_dopparse(ad):\n",
    "    try:\n",
    "        dopparse(ad)\n",
    "    except:\n",
    "        print('hello')\n",
    "        mistakes.append(ad)\n",
    "file_name = []\n",
    "for paths in tqdm(os.listdir('content/content')):\n",
    "    for path in os.listdir('content/content' + '/' + paths):\n",
    "        file_name.append(paths +'/' + path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_urls = defaultdict(str)\n",
    "with open('urls.numerate.txt') as f:\n",
    "    for url in f:\n",
    "        line = url[:-1].split('\\t')\n",
    "        docid_urls[int(line[0])] = line[1]\n",
    "        \n",
    "url_docname = defaultdict(str)\n",
    "for paths in tqdm(os.listdir('content/content')):\n",
    "    for path in os.listdir('content/content' + '/' + paths):\n",
    "        with codecs.open('content/content/' + paths + '/' + path,encoding = 'utf-8') as f:\n",
    "            url_docname[next(f)[:-1]] = 'content/content/' + paths + '/' + path\n",
    "# with open('url_docname.pkl','wb') as f:\n",
    "#     f.dump(url_docname)\n",
    "# with open('docid_urls.pkl','wb') as f:\n",
    "#     f.dump(docid_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = punctuation + '«»©—. …”'\n",
    "punctuation = escape(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('doc_to_text.pkl','wb') as f:\n",
    "#     pickle.dump(doc_to_text,f)\n",
    "\n",
    "# with open('doc_to_text.pkl','rb') as f:\n",
    "#     doc_to_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_mistakes(string):\n",
    "    req = requests.post('https://speller.yandex.net/services/spellservice.json/checkText',data = {'text':string})\n",
    "    new_string = string\n",
    "    minus_len = 0\n",
    "    for word in req.json():\n",
    "        new_string = new_string[:word['pos'] - minus_len] + word['s'][0] + new_string[word['pos'] - minus_len + word['len']:]\n",
    "        minus_len += word['len'] - len(word['s'][0]) \n",
    "    return new_string\n",
    "def russ(word):\n",
    "    lit=set('абвгдеёжзийклмнопрстуфхцчшщъыьэюяАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ1234567890')\n",
    "    if word[0] not in lit:\n",
    "        return False\n",
    "    return True\n",
    "id_queries = defaultdict(str)\n",
    "with codecs.open('queries.numerate.txt',encoding = 'utf-8') as f:\n",
    "    for url in tqdm(f):\n",
    "        line = url[:-1].split('\\t')\n",
    "        id_queries[int(line[0])] = change_mistakes(line[1])\n",
    "all_words = set([word for x in id_queries.values() for word in x.split()])\n",
    "eng_words = []\n",
    "for word in all_words:\n",
    "    if not russ(word) and word not in ['http','ru','com'] and len(word)>1:\n",
    "        eng_words.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def Parse(ad1):\n",
    "    lit=set('qwertyuiopasdfghjklzxcvbnm')\n",
    "    file = 'content/content/{}'.format(ad1)\n",
    "    f = codecs.open(file, encoding='utf-8',errors = 'ignore')\n",
    "    soup = BeautifulSoup(f, 'lxml')\n",
    "    cont = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, cont)\n",
    "    i = ' '.join(visible_texts)\n",
    "    result = []\n",
    "    a=i.strip()\n",
    "    key = lambda x: x if(x not in punctuation) else ' '   \n",
    "    res=[''.join([key(c) for c in word]).lower() for word in a.split()]\n",
    "    for word in res:\n",
    "        flag = 1\n",
    "        if word in eng_words:\n",
    "            result.append(word.lower())\n",
    "            continue\n",
    "        counter = 0\n",
    "        for j in word.lower():\n",
    "            if (j not in lit):\n",
    "                counter += 1\n",
    "        if counter == len(word):\n",
    "            result.append(word.lower())\n",
    "#     request = global_mistakes(' '.join(result)).split()\n",
    "#     result = [morph.parse(word)[0].normal_form for word in request]\n",
    "    doc_to_text[file] = ' '.join(result)\n",
    "@illustration\n",
    "def final_Parse(ad):\n",
    "    try:\n",
    "        Parse(ad)\n",
    "    except:\n",
    "        print('hello')\n",
    "        mistakes.append(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_text = defaultdict(str)\n",
    "mistakes = []\n",
    "with ThreadPool(10) as pool: \n",
    "    pool.map(final_Parse, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_to_text.pkl','wb') as f:\n",
    "    pickle.dump(doc_to_text,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_sub = [r'\\xad',r'…',r'\\u200b',r'¦',r'”',r'“','→','★']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for key in tqdm(doc_to_text.keys()):\n",
    "    for word in doc_to_text[key].split():\n",
    "        new_word = word\n",
    "        for s in for_sub:\n",
    "            new_word = re.sub(s,'',new_word)\n",
    "        words.add(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_lemma = {}\n",
    "@illustration\n",
    "def lemma_for_word(word):\n",
    "    dict_for_lemma[word] = morph.parse(word)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_for_lemma = {}\n",
    "# with ThreadPool(10) as pool: \n",
    "#     pool.map(lemma_for_word, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_lemma_cirkle = defaultdict(str)\n",
    "for i in tqdm(words):\n",
    "    dict_for_lemma_cirkle[i] = morph.parse(i)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_text_new = {}\n",
    "def test_lemma(ad1):\n",
    "    try:\n",
    "        file = 'content/content/{}'.format(ad1)\n",
    "        request = doc_to_text[file].split()\n",
    "        result = ' '.join([dict_for_lemma_cirkle[word] for word in request if word])\n",
    "        doc_to_text_new[ad1] =  result\n",
    "    except:\n",
    "        print(ad1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in tqdm(file_name[:1000]):\n",
    "    test_lemma(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemms = set()\n",
    "with codecs.open('Полная парадигма. Морфология. Орфоэпия. Частотность.txt',encoding = 'Windows-1251') as f:\n",
    "    i = 0\n",
    "    for line in tqdm(f):\n",
    "        i+=1\n",
    "        if line[0] != ' ':\n",
    "            for k in line.split('|')[0].split():\n",
    "                all_lemms.add(k)\n",
    "lems = set()\n",
    "for word in tqdm(all_lemms):\n",
    "    lems.add(morph.parse(word)[0].normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lemms = set(dict_for_lemma_cirkle.values())\n",
    "counter = 0\n",
    "er = set()\n",
    "for word in tqdm(my_lemms):\n",
    "    if (word not in lems) and not word.isdigit():\n",
    "        counter  += 1\n",
    "        er.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illustration(func):\n",
    "    \"\"\"\n",
    "    Распаралеливание выкачки страниц.\n",
    "    \"\"\"\n",
    "    mutex = ThreadLock()\n",
    "    n_thread = ThreadValue('i',0)\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **argv):\n",
    "        result = func(*args, **argv)\n",
    "        with mutex:\n",
    "            nonlocal n_thread\n",
    "            n_thread.value +=1\n",
    "            if n_thread.value % 10 == 0:\n",
    "                print(f\"\\r{n_thread.value} objects are processed...\",end ='',flush = True)\n",
    "        return result\n",
    "    return wrapper\n",
    "mistakes = []\n",
    "@illustration\n",
    "def change_mistakes(string):\n",
    "    try:\n",
    "        req = requests.post('https://speller.yandex.net/services/spellservice.json/checkText',data = {'text':string})\n",
    "        new_string = string\n",
    "        minus_len = 0\n",
    "        for word in req.json():\n",
    "            new_string = new_string[:word['pos'] - minus_len] + word['s'][0] + new_string[word['pos'] - minus_len + word['len']:]\n",
    "            minus_len += word['len'] - len(word['s'][0]) \n",
    "        error[string] = new_string\n",
    "    except:\n",
    "        print('ERROR!')\n",
    "        mistakes.append(string)\n",
    "error = {}\n",
    "with ThreadPool(20) as pool: \n",
    "    pool.map(change_mistakes, er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('error.pkl','wb') as f:\n",
    "#     pickle.dump(error,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPool(20) as pool: \n",
    "    pool.map(change_mistakes, mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_n = set()\n",
    "for i in mistakes:\n",
    "    if i not in error:\n",
    "        mistakes_n.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_count = 0\n",
    "real_error = {}\n",
    "for k in error.keys():\n",
    "    if k != error[k]:\n",
    "        er_count += 1\n",
    "        real_error[k] = error[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_files = []\n",
    "for i in file_name:\n",
    "    if 'content/content/' + i not in doc_to_text:\n",
    "        print(i)\n",
    "        er_files.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "for key in tqdm(doc_to_text.keys()):\n",
    "    for word in doc_to_text[key].split():\n",
    "        new_word = word\n",
    "        for s in for_sub:\n",
    "            new_word = re.sub(s,'',new_word)\n",
    "        words.add(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words.pkl','wb') as f:\n",
    "    pickle.dump(words,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(words):\n",
    "    dict_for_lemma_cirkle[i] = morph.parse(i)[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemms.pkl','wb') as f:\n",
    "    pickle.dump(dict_for_lemma_cirkle,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lemms = set(dict_for_lemma_cirkle.values())\n",
    "counter = 0\n",
    "er = set()\n",
    "for word in tqdm(my_lemms):\n",
    "    if (word not in lems) and not word.isdigit() and word not in real_error:\n",
    "        counter  += 1\n",
    "        er.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_er = set()\n",
    "for e in er:\n",
    "    if e not in error:\n",
    "        new_er.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(new_er):\n",
    "    try:\n",
    "        change_mistakes(i)\n",
    "    except:\n",
    "        print('Error!',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er_count = 0\n",
    "real_error = {}\n",
    "for k in error.keys():\n",
    "    if k != error[k]:\n",
    "        er_count += 1\n",
    "        real_error[k] = error[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('real_error.pkl','wb') as f:\n",
    "    pickle.dump(real_error,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ad in tqdm(file_name):\n",
    "    test_lemma(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_error  = {}\n",
    "for word in tqdm(real_error.keys()):\n",
    "    lemma_error[word] = morph.parse(real_error[word])[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemma_error.pkl','wb') as f:\n",
    "    pickle.dump(lemma_error,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lemma_error.items())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemma_error.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = {}\n",
    "def final(ad1):\n",
    "    try:\n",
    "        file = 'content/content/{}'.format(ad1)\n",
    "        request = doc_to_text_new[ad1].split()\n",
    "        key = lambda x: lemma_error[x] if x in lemma_error else x\n",
    "        result = ' '.join([key(word) for word in request if word])\n",
    "        final_text[ad1] =  result\n",
    "    except:\n",
    "        print(ad1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(file_name):\n",
    "    final(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_text.pkl','wb') as f:\n",
    "    pickle.dump(final_text,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text[file_name[13]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
